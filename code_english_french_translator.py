# -*- coding: utf-8 -*-
"""Code_English-French-Translator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17EVUImfxGoV4Ki2hJt7iTvuvvLPbuE5-

# <center><u> <h1>English French-Translator</h1></u></center>

Savez-vous comment créer une application de traduction de langue?<br>
Did you understand the above sentence?<br>
Well after googling it, I found its meaning as:<br>
Do you know how to create a language translator app?<br>

We all know about Google Translate which allows us to convert from one language to another and it’s very useful for learning and understanding new languages.
<br>
<br>


![](https://daleonai.com/images/2019-11-05-improving-machine-translation-with-the-google-translation-api-advanced/1.png)


In this project we aim to convert English phrases to French using RNN on Deep Learning Neural Network

#Introduction
In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.

Preprocess - You'll convert text to sequence of integers.
Models Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!
Prediction Run the model on English text.

Now let's start by importing necessary libraries.
- Importing numpy for working with arrays
-It then defines a Tokenizer object that will be used to split text into individual words using tensorflow tokenizer.
Refer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer
- pad_sequences is used to ensure that all sequences in a list have the same length.
Refer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences
-Keras model represents the actual neural network model.
-the Sequential model is a linear stack of layers.
-Importing GRU,Input,Dense,TimeDistributed,Activation,RepeatVector,Bidirectional,Dropout,LSTM,Embedding from tensorflow layers.
Refer:https://www.tensorflow.org/api_docs/python/tf/keras/layers
"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model , Sequential
from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM , Embedding
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
import tensorflow as tf

"""#Load Data
The data is located in data/small_vocab_en and data/small_vocab_fr. The small_vocab_en file contains English sentences with their French translations in the small_vocab_fr file. Load the English and French data from these files from running the cell below.
"""

from google.colab import drive
drive.mount('/content/drive')

english_data = '/content/drive/MyDrive/Dataset/small_vocab_en.txt'
french_data = '/content/drive/MyDrive/Dataset/small_vocab_fr.txt'

"""- The OS module in Python provides functions for interacting with the operating system
- The code loads the data from a file called input_file. The code then splits the string of text into an array using split(). <br>
Then, it uses list comprehension to create a list with each line in the array as its own element.
"""

import os

def load_data(path):
    input_file = os.path.join(path)
    with open(input_file, "r") as f:
        data = f.read()

    return data.split('\n')

"""Now loading all english and french data into variables."""

english_sentences = load_data(english_data)
french_sentences = load_data(french_data)

"""#Analysis of Dataset
Let us look at a few examples in the dataset of both language
"""

for i in range(5):
  print('Sample :', i)
  print(english_sentences[i])
  print(french_sentences[i])
  print('-'*50)

"""#Convert to Vocabulary
The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with.

A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.<br>
Refer:https://docs.python.org/3/library/collections.html
"""

#importing collections
import collections

#we will check for english vocabulary
#first we will iterate through english sentence
#then we will split that words and then we will use counter function
#refer more information in documentation
#we done for english vocab
#english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])
english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])
print('English Vocab:', len(english_words_counter))
french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])
print('French Vocab:', len(french_words_counter))

"""#Tokenize (IMPLEMENTATION)
For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like "dog" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be numbers.

We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.

Turn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below.
"""

def tokenize(x):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(x)
  return tokenizer.texts_to_sequences(x), tokenizer

"""- The code starts by tokenizing the text_sentences list into individual sentences.Then, it prints out the word index of each sentence in the text_tokenized list.<br>
Next, it iterates through each sentence and prints out a sample output for that sentence.
"""

# Tokenize Sample output
text_sentences = [
    'The quick brown fox jumps over the lazy dog .',
    'By Jove , my quick study of lexicography won a prize .',
    'This is a short sentence .']

#pass the sample text into tokenize[text_tokenized, text_tokenizer = tokenize(text_sentences)]
text_tokenized, text_tokenizer  = tokenize(text_sentences)
#print text_tokenizer.word_index
print(text_tokenizer.word_index)
print()
#iterate over sample text and text_tokeinzed
for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):
  print('Sequence {} in x'.format(sample_i + 1))
  print('  Input:  {}'.format(sent))
  print('  Output: {}'.format(token_sent))

"""#Padding (IMPLEMENTATION)
When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.

Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function.
"""

def pad(x, length=None):
  return pad_sequences(x, maxlen=length, padding='post')

"""- The code is used to preprocess the input data set. - The tokenize function splits the text into individual tokens, which are then passed to a function called pad that takes in a list of tokens and pads them with a specified character (in this case, spaces)."""

def preprocess(x, y):
  preprocess_x, x_tk = tokenize(x)
  preprocess_y, y_tk = tokenize(y)
    #padding the data x
  preprocess_x = pad(preprocess_x)
    #padding the data y
  preprocess_y = pad(preprocess_y)
    #padding the data y


    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions
    #Expanding dimensions
  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)

  return preprocess_x, preprocess_y, x_tk, y_tk

    #return preprocess_x, preprocess_y, x_tk, y_tk


#preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\
    #preprocess(english_sentences, french_sentences)
preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\
preprocess(english_sentences, french_sentences)

#print Max English sentence length
max_english_sequence_length = preproc_english_sentences.shape[1]

#print Max french sentence length
max_french_sequence_length = preproc_french_sentences.shape[1]
#print len of english vocabulary
english_vocab_size = len(english_tokenizer.word_index)
#print len of french vocabulary
french_vocab_size = len(french_tokenizer.word_index)

print('Data Preprocessed')
print("Max English sentence length:", max_english_sequence_length)
print("Max French sentence length:", max_french_sequence_length)
print("English vocabulary size:", english_vocab_size)
print("French vocabulary size:", french_vocab_size)

"""#Create Model

In this section, you will experiment with various neural network architectures. You will begin by training four relatively simple architectures.

Model 1 is a simple RNN
Model 2 is a RNN with Embedding
Model 3 is a Bidirectional RNN
Model 4 is an optional Encoder-Decoder RNN
After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.

## Ids Back to Text
The neural network will be translating the input to words ids, which isn't the final form we want. We want the French translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation. You'll be using this function to better understand the output of the neural network.
"""

def logits_to_text(logits, tokenizer):
  index_to_words = {id: word for word, id in tokenizer.word_index.items()}
  index_to_words[0] = '<PAD>'
  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

"""![Model](https://github.com/tommytracey/AIND-Capstone/raw/8267d4fe72e48c595a0aff46eaf0a805fff0f36d/images/embedding.png)

#Building Model
Here we use RNN model combined with GRU nodes for translation

- The code starts by defining the input shape and output sequence length.Next, it defines the number of unique English words in the dataset and French words in the dataset.The code then builds a Keras model using word embedding on x and y.It also sets hyperparameters for learning rate, which is 0.005, as well as building layers for this model.Finally, it compiles this model with sparse_categorical_crossentropy loss function and Adam optimizer with learning rate set to 0.005. The code will create a Keras model that has been trained to recognize words in English and French.
"""

def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):
    """
    Build and train a RNN model using word embedding on x and y
    :param input_shape: Tuple of input shape
    :param output_sequence_length: Length of output sequence
    :param english_vocab_size: Number of unique English words in the dataset
    :param french_vocab_size: Number of unique French words in the dataset
    :return: Keras model built, but not trained
    """
    # TODO: Implement

    # Hyperparameters
    learning_rate = 0.005

    # TODO: Build the layers
    #create model Sequential
    model = Sequential() # Changed 'Model' to 'model'
    #add embedding layer with english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]
    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))
    #add GRU layer of 256
    model.add(GRU(256, return_sequences=True))
    #add TimeDistribute layer dense of 1024 and applying of activation function of relu
    model.add(TimeDistributed(Dense(1024, activation='relu')))
    #adding dropout layer
    model.add(Dropout(0.5))
    #add TimeDistributed Dense layer of french_vocab_size and applying softmax activation function
    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))

    # Compile model
    # with loss function of activation function with adam optimizer and accuaray metrix
    model.compile(loss=sparse_categorical_crossentropy,
                  optimizer=Adam(learning_rate),
                  metrics=['accuracy'])
    return model

# Reshaping the input to work with a basic RNN
tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])

tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))

"""Finally calling the model function"""

#calling our model
#with tmp_x.shape, preproc_french_sentences.shape[1], len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)
simple_rnn_model = embed_model(
    tmp_x.shape,
    preproc_french_sentences.shape[1],
    len(english_tokenizer.word_index)+1,
    len(french_tokenizer.word_index)+1)

"""Printing model summary"""

#print Model summary
simple_rnn_model.summary()

"""#Training the model
Here we start to train the model and pass the english text and the max_sequence_length, with vocab size for both english and french text
"""

#model train with tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2
history = simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)

"""#Saving our model"""

#save our model
simple_rnn_model.save('simple_rnn_model.h5')

"""#Arbitrary Predictions
Performing predictions on the models using User Input.
"""

english_tokenizer.word_index

#define a function for final_predictions with text timetable.
def final_predictions(text):

  #converting y_ids to words [y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}]
  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}
  y_id_to_word[0] = '<PAD>'


  #spliting our english word and tokezing
  sentence =  [english_tokenizer.word_index[word] for word in text.split()]
  #padding our sentences
  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')
  #predicting our sentence
  text1 = logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer)
  #converting into text

  #avoiding of <PAD> part in output
  #create a variable of text
  text2 = ""
  for i in text1.split():
    if i != '<PAD>':
      break
    else:
      text2 = text2 + " " + i
  return text2

"""#Implementation
Enter your input here to get predictions. We will using Gradio for implementation part. Refer video for detailed information
"""

####Refer Video
from IPython.display import YouTubeVideo
YouTubeVideo('RiCQzBluTxU', width=600, height=300)

pip install gradio

"""After installing, we import gradio as gr"""

import gradio as gr

"""- The code creates an interface with a function called final_predictions. - The inputs of the interface are a textbox that has two lines and a placeholder, which is "Text to translate". - The outputs of the interface are "text". - The code launches the program in debug mode."""

#creating interface for gradio
interface = gr.Interface(
    fn=final_predictions,
    inputs=gr.Textbox(lines=2, placeholder="Text to translate"),
    outputs="text",
    title="English to French Translator")



#lanuch the interface
interface.launch(debug=True)